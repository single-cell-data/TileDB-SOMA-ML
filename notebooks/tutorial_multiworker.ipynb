{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multi-process training\n",
    "\n",
    "Multi-process usage of `tiledbsoma_ml.ExperimentDataset` includes both:\n",
    "* using the [`torch.utils.data.DataLoader`] with 1 or more workers (i.e., with an argument of `n_workers=1` or greater)\n",
    "* using a multi-process training configuration, such as [`DistributedDataParallel`]\n",
    "\n",
    "In these configurations, `ExperimentDataset` will automatically partition data across workers. However, when using `shuffle=True`, there are several things to keep in mind:\n",
    "\n",
    "1. All worker processes must share the same random number generator `seed`, ensuring that all workers shuffle and partition the data in the same way.\n",
    "2. To ensure that each epoch returns a _different_ shuffle, the caller must set the epoch, using the `set_epoch` API. This is identical to the behavior of [`torch.utils.data.distributed.DistributedSampler`].\n",
    "\n",
    "[DataLoader]: https://pytorch.org/docs/stable/data.html\n",
    "[`torch.utils.data.DataLoader`]: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "[`torch.utils.data.distributed.DistributedSampler`]: https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler\n",
    "[`DistributedDataParallel`]: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Papermill] parameters:\n",
    "\n",
    "[Papermill]: https://papermill.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tissue = \"tongue\"\n",
    "n_epochs = 20\n",
    "census_version = \"2024-07-01\"\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiledbsoma import AxisQuery, Experiment, SOMATileDBContext\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tiledbsoma_ml import ExperimentDataset\n",
    "\n",
    "CZI_Census_Homo_Sapiens_URL = f\"s3://cellxgene-census-public-us-west-2/cell-census/{census_version}/soma/census_data/homo_sapiens/\"\n",
    "\n",
    "experiment = Experiment.open(\n",
    "    CZI_Census_Homo_Sapiens_URL,\n",
    "    context=SOMATileDBContext(tiledb_config={\"vfs.s3.region\": \"us-west-2\", \"vfs.s3.no_sign_request\": \"true\"}),\n",
    ")\n",
    "obs_value_filter = f\"tissue_general == '{tissue}' and is_primary_data == True\"\n",
    "\n",
    "with experiment.axis_query(\n",
    "    measurement_name=\"RNA\", obs_query=AxisQuery(value_filter=obs_value_filter)\n",
    ") as query:\n",
    "    obs_df = query.obs(column_names=[\"cell_type\"]).concat().to_pandas()\n",
    "    cell_type_encoder = LabelEncoder().fit(obs_df[\"cell_type\"].unique())\n",
    "\n",
    "experiment_dataset = ExperimentDataset(\n",
    "    query,\n",
    "    layer_name=\"raw\",\n",
    "    obs_column_names=[\"cell_type\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def train_epoch(model, train_dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for X_batch, obs_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = torch.from_numpy(X_batch).float().to(device)\n",
    "\n",
    "        # Perform prediction\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # Determine the predicted label\n",
    "        probabilities = torch.nn.functional.softmax(outputs, 1)\n",
    "        predictions = torch.argmax(probabilities, axis=1)\n",
    "\n",
    "        # Compute the loss and perform back propagation\n",
    "        obs_batch = torch.from_numpy(cell_type_encoder.transform(obs_batch['cell_type'])).to(device)\n",
    "        train_correct += (predictions == obs_batch).sum().item()\n",
    "        train_total += len(predictions)\n",
    "\n",
    "        loss = loss_fn(outputs, obs_batch.long())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= train_total\n",
    "    train_accuracy = train_correct / train_total\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-worker DataLoader\n",
    "\n",
    "If you use a multi-worker data loader (i.e., `num_workers` with a value other than `0`), and `shuffle=True`, remember to call `set_epoch` at the start of each epoch, _before_ the iterator is created.\n",
    "\n",
    "The same approach should be taken for parallel training, e.g., when using DDP or DP.\n",
    "\n",
    "*Tip*: when running with `num_workers=0`, i.e., using the data loader in-process, the `ExperimentDataset` will automatically increment the epoch count each time the iterator completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "switching torch multiprocessing start method from \"fork\" to \"spawn\"\n"
     ]
    }
   ],
   "source": [
    "from tiledbsoma_ml import experiment_dataloader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# The size of the input dimension is the number of genes\n",
    "input_dim = experiment_dataset.shape[1]\n",
    "\n",
    "# The size of the output dimension is the number of distinct cell_type values\n",
    "output_dim = len(cell_type_encoder.classes_)\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a two-worker data loader. The dataset is shuffled, so call `set_epoch` to ensure\n",
    "# that a different shuffle is applied on each epoch.\n",
    "dataloader = experiment_dataloader(\n",
    "    experiment_dataset, num_workers=num_workers, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0165012 Accuracy 0.3866\n",
      "Epoch 2: Train Loss: 0.0148111 Accuracy 0.4217\n",
      "Epoch 3: Train Loss: 0.0144168 Accuracy 0.6109\n",
      "Epoch 4: Train Loss: 0.0141248 Accuracy 0.8374\n",
      "Epoch 5: Train Loss: 0.0138151 Accuracy 0.9001\n",
      "Epoch 6: Train Loss: 0.0136300 Accuracy 0.9123\n",
      "Epoch 7: Train Loss: 0.0135218 Accuracy 0.9234\n",
      "Epoch 8: Train Loss: 0.0134472 Accuracy 0.9324\n",
      "Epoch 9: Train Loss: 0.0133907 Accuracy 0.9375\n",
      "Epoch 10: Train Loss: 0.0133443 Accuracy 0.9419\n",
      "Epoch 11: Train Loss: 0.0132998 Accuracy 0.9456\n",
      "Epoch 12: Train Loss: 0.0132594 Accuracy 0.9489\n",
      "Epoch 13: Train Loss: 0.0132298 Accuracy 0.9524\n",
      "Epoch 14: Train Loss: 0.0132037 Accuracy 0.9549\n",
      "Epoch 15: Train Loss: 0.0131809 Accuracy 0.9568\n",
      "Epoch 16: Train Loss: 0.0131603 Accuracy 0.9585\n",
      "Epoch 17: Train Loss: 0.0131425 Accuracy 0.9601\n",
      "Epoch 18: Train Loss: 0.0131270 Accuracy 0.9613\n",
      "Epoch 19: Train Loss: 0.0131112 Accuracy 0.9630\n",
      "Epoch 20: Train Loss: 0.0130966 Accuracy 0.9639\n",
      "CPU times: user 1min 6s, sys: 1min 58s, total: 3min 4s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(n_epochs):\n",
    "    experiment_dataset.set_epoch(epoch)\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model, dataloader, loss_fn, optimizer, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: Train Loss: {train_loss:.7f} Accuracy {train_accuracy:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
