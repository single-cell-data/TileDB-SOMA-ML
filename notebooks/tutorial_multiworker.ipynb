{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-process training\n",
    "\n",
    "Multi-process usage of `tiledbsoma_ml.ExperimentAxisQueryIterDataset` includes both:\n",
    "* using the [`torch.utils.data.DataLoader`] with 1 or more workers (i.e., with an argument of `n_workers=1` or greater)\n",
    "* using a multi-process training configuration, such as [`DistributedDataParallel`]\n",
    "\n",
    "In these configurations, `ExperimentAxisQueryIterDataset` will automatically partition data across workers. However, when using `shuffle=True`, there are several things to keep in mind:\n",
    "\n",
    "1. All worker processes must share the same random number generator `seed`, ensuring that all workers shuffle and partition the data in the same way.\n",
    "2. To ensure that each epoch returns a _different_ shuffle, the caller must set the epoch, using the `set_epoch` API. This is identical to the behavior of [`torch.utils.data.distributed.DistributedSampler`].\n",
    "\n",
    "[DataLoader]: https://pytorch.org/docs/stable/data.html\n",
    "[`torch.utils.data.DataLoader`]: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "[`torch.utils.data.distributed.DistributedSampler`]: https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler\n",
    "[`DistributedDataParallel`]: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import tiledbsoma as soma\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tiledbsoma_ml as soma_ml\n",
    "\n",
    "CZI_Census_Homo_Sapiens_URL = \"s3://cellxgene-census-public-us-west-2/cell-census/2024-07-01/soma/census_data/homo_sapiens/\"\n",
    "\n",
    "experiment = soma.open(\n",
    "    CZI_Census_Homo_Sapiens_URL,\n",
    "    context=soma.SOMATileDBContext(tiledb_config={\"vfs.s3.region\": \"us-west-2\", \"vfs.s3.no_sign_request\": \"true\"}),\n",
    ")\n",
    "obs_value_filter = \"tissue_general == 'tongue' and is_primary_data == True\"\n",
    "\n",
    "with experiment.axis_query(\n",
    "    measurement_name=\"RNA\", obs_query=soma.AxisQuery(value_filter=obs_value_filter)\n",
    ") as query:\n",
    "    obs_df = query.obs(column_names=[\"cell_type\"]).concat().to_pandas()\n",
    "    cell_type_encoder = LabelEncoder().fit(obs_df[\"cell_type\"].unique())\n",
    "\n",
    "    experiment_dataset = soma_ml.ExperimentDataset(\n",
    "        query,\n",
    "        layer_name=\"raw\",\n",
    "        obs_column_names=[\"cell_type\"],\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()  # noqa: UP008\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def train_epoch(model, train_dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch = torch.from_numpy(X_batch).float().to(device)\n",
    "\n",
    "        # Perform prediction\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # Determine the predicted label\n",
    "        probabilities = torch.nn.functional.softmax(outputs, 1)\n",
    "        predictions = torch.argmax(probabilities, axis=1)\n",
    "\n",
    "        # Compute the loss and perform back propagation\n",
    "        y_batch = torch.from_numpy(cell_type_encoder.transform(y_batch['cell_type'])).to(device)\n",
    "        train_correct += (predictions == y_batch).sum().item()\n",
    "        train_total += len(predictions)\n",
    "\n",
    "        loss = loss_fn(outputs, y_batch.long())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= train_total\n",
    "    train_accuracy = train_correct / train_total\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-worker DataLoader\n",
    "\n",
    "If you use a multi-worker data loader (i.e., `num_workers` with a value other than `0`), and `shuffle=True`, remember to call `set_epoch` at the start of each epoch, _before_ the iterator is created.\n",
    "\n",
    "The same approach should be taken for parallel training, e.g., when using DDP or DP.\n",
    "\n",
    "*Tip*: when running with `num_workers=0`, i.e., using the data loader in-process, the `ExperimentAxisQueryIterDataset` will automatically increment the epoch count each time the iterator completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "switching torch multiprocessing start method from \"fork\" to \"spawn\"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# The size of the input dimension is the number of genes\n",
    "input_dim = experiment_dataset.shape[1]\n",
    "\n",
    "# The size of the output dimension is the number of distinct cell_type values\n",
    "output_dim = len(cell_type_encoder.classes_)\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-05)\n",
    "\n",
    "# define a two-worker data loader. The dataset is shuffled, so call `set_epoch` to ensure\n",
    "# that a different shuffle is applied on each epoch.\n",
    "experiment_dataloader = soma_ml.experiment_dataloader(\n",
    "    experiment_dataset, num_workers=2, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n",
      "/home/ubuntu/.pyenv/versions/3.12.7/lib/python3.12/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0163469 Accuracy 0.3040\n",
      "Epoch 2: Train Loss: 0.0147538 Accuracy 0.4680\n",
      "Epoch 3: Train Loss: 0.0143231 Accuracy 0.5575\n",
      "Epoch 4: Train Loss: 0.0140254 Accuracy 0.6792\n",
      "Epoch 5: Train Loss: 0.0138131 Accuracy 0.7943\n",
      "Epoch 6: Train Loss: 0.0136806 Accuracy 0.8565\n",
      "Epoch 7: Train Loss: 0.0135832 Accuracy 0.8925\n",
      "Epoch 8: Train Loss: 0.0134801 Accuracy 0.9130\n",
      "Epoch 9: Train Loss: 0.0134058 Accuracy 0.9217\n",
      "Epoch 10: Train Loss: 0.0133559 Accuracy 0.9272\n",
      "Epoch 11: Train Loss: 0.0133088 Accuracy 0.9336\n",
      "Epoch 12: Train Loss: 0.0132692 Accuracy 0.9406\n",
      "Epoch 13: Train Loss: 0.0132350 Accuracy 0.9451\n",
      "Epoch 14: Train Loss: 0.0132063 Accuracy 0.9495\n",
      "Epoch 15: Train Loss: 0.0131819 Accuracy 0.9535\n",
      "Epoch 16: Train Loss: 0.0131599 Accuracy 0.9556\n",
      "Epoch 17: Train Loss: 0.0131419 Accuracy 0.9585\n",
      "Epoch 18: Train Loss: 0.0131233 Accuracy 0.9605\n",
      "Epoch 19: Train Loss: 0.0131053 Accuracy 0.9625\n",
      "Epoch 20: Train Loss: 0.0130904 Accuracy 0.9638\n",
      "CPU times: user 1min 13s, sys: 1min 11s, total: 2min 25s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(20):\n",
    "    experiment_dataset.set_epoch(epoch)\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model, experiment_dataloader, loss_fn, optimizer, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: Train Loss: {train_loss:.7f} Accuracy {train_accuracy:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
